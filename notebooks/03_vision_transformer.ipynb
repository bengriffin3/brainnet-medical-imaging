{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'brainnet-env (Python 3.8.8)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import pathlib\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Download dataset using kagglehub\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"masoudnickparvar/brain-tumor-mri-dataset\")\n",
    "data_dir = pathlib.Path(path)\n",
    "print(f\"Path to dataset files: {data_dir}\")\n",
    "\n",
    "# Define transformations for the input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale images to 3-channel RGB (required by ViT)\n",
    "    transforms.Resize((224, 224)),  # Resize all images to 224x224 pixels\n",
    "    transforms.ToTensor(),  # Convert image to a tensor with values in [0, 1]\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize pixel values to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load training and validation datasets using torchvision's ImageFolder \n",
    "train_set = torchvision.datasets.ImageFolder(data_dir.joinpath('Training'), transform=transform)\n",
    "val_set = torchvision.datasets.ImageFolder(data_dir.joinpath('Testing'), transform=transform)\n",
    "\n",
    "# Create DataLoader objects to enable batch processing\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "# Vision Transformer (ViT) Implementation\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "         # (i) Calculating the number of patches\n",
    "        self.num_patches = (img_size // patch_size) ** 2  # Total number of patches (14x14 = 196 for a 224x224 image)\n",
    "        # (iii) Creating the projection layer\n",
    "        self.projection = nn.Conv2d(in_channels,       # 3 channels (RGB)\n",
    "                                    embed_dim,         # The embedding dimension (size of the resulting vector)\n",
    "                                    kernel_size=patch_size,  # Size of the patches (16x16)\n",
    "                                    stride=patch_size)       # Set to be the same as kernel_size to ensure Non-overlapping patches\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (i) Splitting the Image into Patches + (iii) Projecting Each Patch to a Vector (Embedding)\n",
    "        x = self.projection(x)  # Produces a tensor of shape [batch_size, embed_dim, 14, 14]\n",
    "\n",
    "        # (ii) Flattening Each Patch\n",
    "        x = x.flatten(2)  # Flatten to [batch_size, embed_dim, num_patches] (14x14 patches = 196)\n",
    "\n",
    "        # (iv) Stacking Patches into a Sequence\n",
    "        x = x.transpose(1, 2)  # Rearrange to [batch_size, num_patches, embed_dim]\n",
    "        return x\n",
    "    \n",
    "#  Step-by-Step Process:\n",
    "    # (i) Splitting the Image into Patches (Non-Overlapping)\n",
    "    # (ii) Flattening Each Patch (Making them 1D)\n",
    "    # (iii) Projecting Each Patch to a Vector (Embedding)\n",
    "    # (iv) Stacking Patches into a Sequence\n",
    "\n",
    "\n",
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, num_patches, embed_dim):\n",
    "        super().__init__()\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        cls_token = nn.Parameter(torch.randn(1, 1, x.size(-1))).to(device)\n",
    "        cls_token = cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = x + self.position_embedding\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.layernorm1(x), self.layernorm1(x), self.layernorm1(x))[0]\n",
    "        x = x + self.mlp(self.layernorm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, num_classes=4, embed_dim=768, num_heads=8, mlp_dim=2048, num_layers=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.position_embedding = PositionEmbedding(self.num_patches, embed_dim)\n",
    "\n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, mlp_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        x = self.position_embedding(x)\n",
    "\n",
    "        for block in self.encoder:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        cls_token = x[:, 0]\n",
    "        return self.classifier(cls_token)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = VisionTransformer().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    val_loss_list.append(val_loss)\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "\n",
    "# Plot Loss and Accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss_list, label='Train Loss')\n",
    "plt.plot(val_loss_list, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracy_list, label='Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainnet-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
