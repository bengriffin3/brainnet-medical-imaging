# -*- coding: utf-8 -*-
"""data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-yvN1D5bnI0WBak5n2Da1h5vxeebP9YQ
"""

# Standard library imports
import os
import pathlib
from typing import Tuple, Dict, List

import kagglehub
import pandas as pd
import shutil
from tqdm import tqdm  # for progress bar (optional)
import hashlib

# PyTorch imports
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
from torch.optim.lr_scheduler import ReduceLROnPlateau

from PIL import Image
import random
import matplotlib.pyplot as plt
import numpy as np

# Set up device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

label_conversion_dict = {
            'glioma': 0,
            'meningioma': 1,
            'notumor': 2,
            'pituitary': 3,
            0: 'glioma',
            1: 'meningioma',
            2: 'notumor',
            3: 'pituitary'
        }

def data_setup():
    # Download dataset (must be assigned or it won't persist)
    dataset_path = kagglehub.dataset_download("masoudnickparvar/brain-tumor-mri-dataset")

    # Create a directory for unique files
    new_path = '/tmp/unique_files/'
    os.makedirs(new_path, exist_ok=True)

    label_conversion_dict = {
        'glioma': 0,
        'meningioma': 1,
        'notumor': 2,
        'pituitary': 3,
        0: 'glioma',
        1: 'meningioma',
        2: 'notumor',
        3: 'pituitary'
    }

    # Collect all file paths in the dataset
    file_paths = []
    for root, _, files in os.walk(dataset_path):
        for file in files:
            file_paths.append(os.path.join(root, file))

    df = pd.DataFrame({'filepath': file_paths})
    df['filename'] = df['filepath'].apply(lambda x: os.path.basename(x))

    # Remove duplicates based on file content (hash)
    df['hash'] = df['filepath'].apply(lambda x: hashlib.md5(open(x, 'rb').read()).hexdigest())
    df_unique = df.drop_duplicates(subset=['hash'], keep='first')

    df_unique['label'] = df_unique['filepath'].apply(lambda x: x.split('/')[-2])
    df_unique['class'] = df_unique['filename'].apply(lambda x: 'train' if x[:2] == 'Tr' else 'test')

    # Copy unique files to new directory
    for filepath in tqdm(df_unique['filepath']):
        filename = os.path.basename(filepath)
        shutil.copy2(filepath, os.path.join(new_path, filename))

    root_dir = '/tmp/brain_tumor_dataset'
    os.makedirs(root_dir, exist_ok=True)

    # Create directories and copy files
    for _, row in df_unique.iterrows():
        class_dir = os.path.join(root_dir, row['class'])
        os.makedirs(class_dir, exist_ok=True)
        shutil.copy2(row['filepath'], os.path.join(class_dir, row['filename']))

    train_dir = os.path.join(root_dir, 'train')
    test_dir = os.path.join(root_dir, 'test')
    os.makedirs(train_dir, exist_ok=True)
    os.makedirs(test_dir, exist_ok=True)

    train_class_dir = os.path.join(train_dir, 'images')
    test_class_dir = os.path.join(test_dir, 'images')
    os.makedirs(train_class_dir, exist_ok=True)
    os.makedirs(test_class_dir, exist_ok=True)

    # Copy files to respective directories
    for _, row in df_unique.iterrows():
        target_dir = train_class_dir if row['class'] == 'train' else test_class_dir
        shutil.copy2(row['filepath'], os.path.join(target_dir, row['filename']))

    transform = get_transform()

    # Create ImageFolder datasets
    train_set = torchvision.datasets.ImageFolder(root=train_dir, transform=transform)
    test_set = torchvision.datasets.ImageFolder(root=test_dir, transform=transform)

    # Create a filename-to-label dictionary
    file_to_label_train = dict(zip(df_unique[df_unique['class'] == 'train']['filename'],
                                   df_unique[df_unique['class'] == 'train']['label']))
    file_to_label_test = dict(zip(df_unique[df_unique['class'] == 'test']['filename'],
                                  df_unique[df_unique['class'] == 'test']['label']))

    # Override the labels with error handling
    train_set.samples = []
    for path, _ in train_set.samples:
        try:
            filename = os.path.basename(path)
            label = file_to_label_train[filename]
            train_set.samples.append((path, label_conversion_dict[label]))
        except KeyError as e:
            print(f"KeyError: {e} for file {path}. Skipping this file.")
            continue

    # Verify if there are valid entries in train_set.samples
    print(f"Total samples in train set after overriding labels: {len(train_set.samples)}")

    train_set.targets = [s[1] for s in train_set.samples]

    test_set.samples = []
    for path, _ in test_set.samples:
        try:
            filename = os.path.basename(path)
            label = file_to_label_test[filename]
            test_set.samples.append((path, label_conversion_dict[label]))
        except KeyError as e:
            print(f"KeyError: {e} for file {path}. Skipping this file.")
            continue

    # Verify if there are valid entries in test_set.samples
    print(f"Total samples in test set after overriding labels: {len(test_set.samples)}")

    test_set.targets = [s[1] for s in test_set.samples]

    # Verify total files and target values
    print(f"Total files in train set: {len(train_set)}, with target values: {list(set(train_set.targets))}")
    print(f"Total files in test set: {len(test_set)}, with target values: {list(set(test_set.targets))}")

    return train_set, test_set, label_conversion_dict



def data_loader(train_set,test_set, batch_size = 64):

    batch_size = batch_size

    # Create data loaders
    train_loader = DataLoader(
            train_set,
            batch_size=batch_size,
            shuffle=True,
            num_workers=2,
            pin_memory=True
        )
    test_loader = DataLoader(
            test_set,
            batch_size=batch_size,
            shuffle=False,
            num_workers=2,
            pin_memory=True
        )

    return train_loader, test_loader



def display_sample(train_set):

    # Randomly sample from dataset
    sample_indices = random.sample(range(len(train_set)), 9)

    # Plot examples
    fig, axes = plt.subplots(3, 3, figsize=(4, 4))
    for ax, idx in zip(axes.flatten(), sample_indices):
        img, label = train_set[idx]
        img_np = np.array(img)
        ax.imshow(img_np.squeeze(), cmap='gray')
        ax.set_title(f"Label: {label_conversion_dict[label]}", fontsize=8)
        ax.axis('off')
    plt.tight_layout()
    plt.show()

# train_set, test_set = data_setup()


def get_transform():
    """
    Get the transformation pipeline for training data.
    
    Note: Currently identical to validation transforms, but kept separate because
    training transforms will later be enhanced with data augmentation techniques
    (e.g., random flips, rotations, scaling) to improve model robustness and
    prevent overfitting.
    
    Returns:
        transforms.Compose: Composition of transforms
    """
    return transforms.Compose([
    transforms.Grayscale(num_output_channels=1),  # Convert to greyscale (i.e., single channel)
    transforms.Resize((128,128)),                 # Resize images to 128x128 pixels
    transforms.ToTensor(),                        # Convert images to PyTorch tensors
    transforms.Normalize(mean=[0.5], std=[0.5])   # Normalize pixel values
    ])